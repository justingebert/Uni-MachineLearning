{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faires Machine Learning\n",
    "Machine Learning Models werden bei ihrer Erschaffung auf Qualität überprüft - z.B. wird geprüft wie hoch der Anteil korrekter Vorhersagen ist (= accuracy). Man kann die Qualität jedoch noch detailierter betrachten. Wenn wir \"Fairness\" als Qualitätskriterium prüfen sprechen wir von einem \"Fairness Audit\". Ein solcher Fairness Audit wird in diesem Notebook an Hand eines Logistic Regression Models für Kreditwürdigkeit demonstriert.\n",
    "\n",
    "\n",
    "**Aufgabe**:\n",
    "Vollziehe die vorgestellten Schritte nach und erledige dann die am Ende gestellten Reflektionsaufgaben. Wenn du noch Zeit hast, versuche dich an der Übungsaufgabe.\n",
    "\n",
    "---\n",
    "\n",
    "## Logistische Regression: Ist eine Person kreditwürdig?\n",
    "\n",
    "Banken wollen Kredite nur an Personen vergeben, die auch kreditwürdig sind. Dafür bauen wir ein Modell, das mit Hilfe von Logistic Regression einschätzt,\n",
    "ob eine Person kreditwürdig ist. Dieses beispielhafte Modell können wir später auf Fairness überprüfen.\n",
    "\n",
    "### Vorbereitung\n",
    "\n",
    "Zunächst wird das verwendete Datenset \"german_credit.csv\" importiert. Dabei werden nur einige Spalten importiert, da das Modell zu diesem Übungszweck sehr simpel gehalten werden soll. Wir verwenden zur Datenverarbeitung die Bibliothek \"pandas\". Mit \"pandas\" kann man tabellarische Daten verarbeiten.\n",
    "\n",
    "- [Dokumentation des Datensets](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))\n",
    "- [Dokumentation Pandas](https://pandas.pydata.org/docs/reference/index.html)\n",
    "\n",
    "Die kategorischen Werte wurden im german_credit.csv bereits in Zahlen umgewandelt. Hier eine kurze Erklärung des Mappings Zahle -> Kategorie pro kategorischer Spalte.\n",
    "\n",
    "- \"Sex & Marital Status\":\n",
    "   - 1: male & separated\n",
    "   - 2: female & separated\n",
    "   - 3: male & single\n",
    "   - 4: male & married/widowed\n",
    "   - 5: female & single (nicht im Datenset enthalten)\n",
    "- \"Value Savings/Stocks\"\n",
    "   - 1:  ... < 100 DM\n",
    "   - 2:  100 <= ... < 500 DM\n",
    "   - 3:  500 <= ... < 1000 DM\n",
    "   - 4:  .. >= 1000 DM\n",
    "   - 5: unknown/ none\n",
    "- \"Payment Status of Previous Credit\"\n",
    "   -  0: no credits taken/ all credits paid back duly\n",
    "    - 1: all credits at this bank paid back duly\n",
    "    - 2: existing credits paid back duly till now\n",
    "    - 3: delay in paying off in the past\n",
    "    - 4: critical account/ other credits existing (not at this bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "demographics = [\"No of dependents\", \"Sex & Marital Status\"] # These columns will be used for the fairness audit - we will check whether the model is fair towards people with a different number of dependents, and whether it is fair towards people of different sex\n",
    "features = [\"Value Savings/Stocks\", \"Payment Status of Previous Credit\", \"Duration of Credit (month)\", \"Account Balance\"] # These columns will be the features for our logistic regression model - we keep it simple for this exercise!\n",
    "target = [\"Creditability\"] # The model should try to predict creditability - that means it classifies each instance in the data set as either creditable or not creditable\n",
    "\n",
    "d = pd.read_csv(\"german_credit.csv\", usecols=np.concatenate([demographics, features, target])) # import the CSV\n",
    "\n",
    "d.head() # show a few rows of the imported data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pre-process the \"Sex & Marital Status\" column to retrieve only the sex of a person, not their marital status.\n",
    "\n",
    "d[\"female\"] = (d[\"Sex & Marital Status\"] == 2) | (d[\"Sex & Marital Status\"] == 5) # is the person female?\n",
    "d[\"female\"] = d[\"female\"].astype(int) # 1 = female, 0 = male\n",
    "d = d.drop(\"Sex & Marital Status\", axis=1) # Remove the column we retrieved the information from since we don't need it anymore\n",
    "demographics = [demographics[0], \"female\"] # Update our demographics list\n",
    "\n",
    "d.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Jetzt trainieren wir ein out-of-the-box Logistic Regression Model mit Hilfe von \"sklearn\". Die Bibliothek \"sklearn\" kann verschiedene Machine Learning Aufgaben übernehmen.\n",
    "\n",
    "- [Dokumentation sklearn](https://scikit-learn.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = d[features + demographics] # x = all columns of the data set that are features or demographics\n",
    "y = np.ravel(d[target].values) # y = the target column of the data set (as a 1d array)\n",
    "\n",
    "x_train_with_demographics, x_test_with_demographics, y_train, y_test = train_test_split(x, y, test_size=0.2) # sklearn can do the train-test split for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the demographics for the training and add them back later on for the fairness audit\n",
    "X_train = x_train_with_demographics[features]\n",
    "X_test = x_test_with_demographics[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "model.fit(X_train, y_train) # Train the logistic regression model on the training instances\n",
    "\n",
    "y_pred = model.predict(X_test) # Classify the test instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einschätzung der Model Qualität\n",
    "\n",
    "Nachdem wir nun beispielhaft ein Logistic Regression Model zur Vorhersage der Kreditwürdigkeit trainiert haben, können wir nun seine Qualität einschätzen.\n",
    "\n",
    "### Qualität allgemein und pro Klasse\n",
    "\n",
    "Wir werfen einen Blick auf den Report, den sklearn uns generiert. Er enthält die folgenden Metriken:\n",
    "- [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) pro target Klasse\n",
    "    - \"precision\" = Von allen Instanzen, die in diese Klasse eingeordnet wurden: Wieviele gehören tatsächlich in diese Klasse?\n",
    "    - \"recall\" = Von allen Instanzen, die tatsächlich in diese Klasse gehören: Wieviele wurden in diese Klasse eingeordnet?\n",
    "- [f1-score](https://en.wikipedia.org/wiki/F-score) pro target Klasse (= harmonisches Mittel von precision und recall)\n",
    "- accuracy (= Klassenunabhängig: Rate der korrekten Klassifizierungen)\n",
    "- die obigen Metriken werden außerdem als Mittelwerte über beide Klassen gegeben\n",
    "    - \"macro\" = ungewichtetes Mittel\n",
    "    - \"weighted\" = gewichtetes Mittel\n",
    "- \"support\" = Anzahl von Instanzen dieser Klasse in y_test\n",
    "\n",
    "Siehe auch: [classification_report documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=[\"not creditable\", \"creditable\"])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness\n",
    "\n",
    "Fairness ist ebenfalls ein Qualitätsfaktor für Machine Learning Modelle. Während es viele Definitionen für Fairness gibt, ist die allgemeine Frage, die dieser Faktor stellt: Funktioniert das Modell gleich gut für alle Gruppen? Diese Frage enthält drei separate Aspekte: \"gleich\", \"gut\" und \"Gruppen\".\n",
    "\n",
    "\"Gut\" bezieht sich auf Qualitäts-Metriken, die aus der [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix) (True Positives, False Positives, True Negatives und False Negatives) berechnet werden - die Auswahl erfolgt je nach verwendeter Fairness-Definition (siehe [Verma & Rubin 2018](https://fairware.cs.umass.edu/papers/Verma.pdf)). Es muss je nach Fall entschieden werden, welche Fairness-Definitionen relevant sind.\n",
    "\n",
    "Es muss außerdem von Fall zu Fall entschieden werden, wie \"gleich\" die Qualität für verschiedene Gruppen sein muss, damit das Modell fair ist. Die Gleichheit der Qualitäts-Metriken pro Gruppe wird entweder durch die Differenz oder das Verhältnis bestimmt.\n",
    "\n",
    "Letztenendes muss auch noch ausgewählt werden, für welche \"Gruppen\" Fairness geprüft wird. Gruppen können z.B. nach demographischen Eigenschaften wie Geschlecht oder Alter gebildet werden.\n",
    "\n",
    "Wir können die Bibliothek \"fairlearn\" benutzen, um einige Qualitäts-Metriken pro Gruppe für das Modell berechnen zu lassen.\n",
    "\n",
    "- [Fairlearn User Guide](https://fairlearn.org/v0.8/user_guide/)\n",
    "- [Fairlearn Dokumentation](https://fairlearn.org/v0.8/api_reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auswahl von Fairness-Definitionen\n",
    "\n",
    "Bei unserem Use Case handelt sich um die Vorhersage von Kreditwürdigkeit.\n",
    "\n",
    "Wir wollen folgende Fairness-Definitionen prüfen (nach [Verma & Rubin 2018](https://fairware.cs.umass.edu/papers/Verma.pdf)):\n",
    "- Predictive Equality (gleiche FPR) = In verschiedenen Gruppen werden ähnlich viele Personen fälschlicherweise als kreditwürdig eingestuft und erhalten damit eine Chance auf einen Kredit\n",
    "- Equal Opportunity (gleiche FNR) = In verschiedenen Gruppen werden ähnlich viele Personen fälschlicherweise als kreditunwürdig eingestuft und erhalten damit nicht die Chance auf einen Kredit\n",
    "- Overall Accuracy Equality (gleiche accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from fairlearn.metrics import MetricFrame, count, false_negative_rate, false_positive_rate\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"false positive rate\": false_positive_rate, # for predictive parity\n",
    "    \"false negative rate\": false_negative_rate, # for equal opportunity\n",
    "    \"count\": count, # how many instances belong to this group?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Berechnung der Fairness-Metriken\n",
    "\n",
    "Wir lassen nun von fairlearn die ausgewählten Metriken berechnen und anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_feature_0 = x_test_with_demographics[demographics[0]] # Number of dependents\n",
    "m_0 = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_feature_0)\n",
    "\n",
    "m_0.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[1, 4],\n",
    "    legend=False,\n",
    "    figsize=[12, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_feature_1 = x_test_with_demographics[demographics[1]] # female\n",
    "m_1 = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_feature_1)\n",
    "\n",
    "m_1.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[1, 4],\n",
    "    legend=False,\n",
    "    figsize=[12, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sind die Modelle gleich gut?\n",
    "\n",
    "Wir haben nun beobachtet, wie akkurat die Modelle pro Gruppe sind, und wie hoch die Fehlerraten sind. Doch sind diese Metriken ähnlich genug, damit die von uns ausgewählten Fairness-Definitionen erfüllt werden? Reminder, unsere Definitionen sind:\n",
    "\n",
    "- Predictive Equality (gleiche FPR)\n",
    "- Equal Opportunity (gleiche FNR)\n",
    "- Overall Accuracy Equality (gleiche accuracy)\n",
    "\n",
    "Die Ähnlichkeit kann man nun entweder mit der Differenz oder der Rate (Anteil) berechnen. Fairlearn übernimmt auch diese Aufgabe für uns. Welche Differenz oder Rate man als \"zu hoch\" einstuft muss man selber festlegen, da es hierzu noch keine Standards gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0.difference() # Number of dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_0.ratio() # Number of dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1.difference() # female?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_1.ratio() # female?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersektionale Gruppen\n",
    "\n",
    "In der Fairness-Forschung hat man herausgefunden, dass Diskriminierung sich oft \"aufaddiert\". D.h. wenn man beobachten kann dass z.B. People of Color diskriminiert werden und Frauen diskriminiert werden, dass dann Women of Color besonders stark diskriminiert werden. \"Women of Color\" ist eine intersektionale Guppe. Wir prüfen daher außerdem noch, wie gut unser Modell für die verschiedenen intersektionalen Gruppen funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_features = x_test_with_demographics[demographics] # both demographics: number of dependents and female\n",
    "m = MetricFrame(metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_features)\n",
    "\n",
    "m.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[1, 4],\n",
    "    legend=False,\n",
    "    figsize=[12, 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflektionsaufgaben\n",
    "\n",
    "- Würdest du das Modell als fair, oder als unfair einschätzen?\n",
    "- Woran liegen womöglich die Unterschiede in Accuracy und Fehlerraten\n",
    "   - zwischen verschiedenen Klassen?\n",
    "   - zwischen verschiedenen (intersektionalen) Gruppen?\n",
    "- Hast du eine Idee, wie man versuchen könnte die Unterschiede anzugleichen?\n",
    "\n",
    "## Übungsaufgabe\n",
    "- Nutze die hier vorgestellten Methoden (soweit möglich) um dein eigenes Klassifizierungsmodell (aus einer früheren Übung in diesem Kurs) zu testen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
